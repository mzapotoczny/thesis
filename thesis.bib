
@article{zaremba_recurrent_2014,
	title = {Recurrent {Neural} {Network} {Regularization}},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	urldate = {2015-02-24},
	journal = {arXiv:1409.2329 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = sep,
	year = {2014},
	note = {00010 
arXiv: 1409.2329},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {2014_Zaremba et al._Recurrent Neural Network Regularization.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\SZBW77R9\\2014_Zaremba et al._Recurrent Neural Network Regularization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\M88A5IX6\\1409.html:text/html}
}

@article{xu_show_2015,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
	shorttitle = {Show, {Attend} and {Tell}},
	url = {http://arxiv.org/abs/1502.03044},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
	urldate = {2015-02-27},
	journal = {arXiv:1502.03044 [cs]},
	author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
	month = feb,
	year = {2015},
	note = {00001 
arXiv: 1502.03044},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {2015_Xu et al._Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\6EUZJFAG\\2015_Xu et al._Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\PNVCQKQE\\1502.html:text/html}
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2014-10-21},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	note = {00171 arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {2013_Mikolov et al._Efficient Estimation of Word Representations in Vector Space.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\Q7A89BGF\\2013_Mikolov et al._Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\ZM98D9T7\\1301.html:text/html}
}

@article{zeiler_adadelta:_2012,
	title = {{ADADELTA}: {An} {Adaptive} {Learning} {Rate} {Method}},
	shorttitle = {{ADADELTA}},
	url = {http://arxiv.org/abs/1212.5701},
	abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	urldate = {2014-09-02},
	journal = {arXiv:1212.5701 [cs]},
	author = {Zeiler, Matthew D.},
	month = dec,
	year = {2012},
	note = {00017 arXiv: 1212.5701},
	keywords = {Computer Science - Learning},
	annote = {Comment: 6 pages},
	file = {2012_Zeiler_ADADELTA An Adaptive Learning Rate Method.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\ABVABU7B\\2012_Zeiler_ADADELTA An Adaptive Learning Rate Method.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\2U3MW33N\\1212.html:text/html}
}

@inproceedings{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://jmlr.org/proceedings/papers/v28/goodfellow13.html},
	urldate = {2014-09-26},
	booktitle = {{ICML}},
	author = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	year = {2013},
	note = {00106},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	pages = {1319--1327},
	file = {2013_Goodfellow et al._Maxout Networks.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\AEU6VFFC\\2013_Goodfellow et al._Maxout Networks.pdf:application/pdf}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2014-09-26},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {00000 arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {2014_Bahdanau et al._Neural Machine Translation by Jointly Learning to Align and Translate.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\EA8QJGP8\\2014_Bahdanau et al._Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\7CU27SJR\\1409.html:text/html}
}

@article{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1409.3215},
	urldate = {2014-09-17},
	journal = {arXiv preprint arXiv:1409.3215},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	year = {2014},
	note = {00000},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
	annote = {Comment: 10 pages},
	file = {2014_Sutskever et al._Sequence to Sequence Learning with Neural Networks.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\SII4Q2D3\\2014_Sutskever et al._Sequence to Sequence Learning with Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\TBRSPBZU\\1409.html:text/html}
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1053-587X},
	doi = {10.1109/78.650093},
	abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, Kuldip K.},
	month = nov,
	year = {1997},
	note = {00157},
	keywords = {artificial data, Artificial neural networks, bidirectional recurrent neural networks, classification experiments, complete symbol sequences, conditional posterior probability, Control systems, Databases, learning by example, learning from examples, negative time direction, parameter estimation, pattern classification, phonemes, positive time direction, probability, real data, recurrent neural nets, Recurrent neural networks, regression experiments, regular recurrent neural network, Shape, Speech processing, speech recognition, statistical analysis, Telecommunication control, TIMIT database, Training, Training data},
	pages = {2673--2681},
	file = {1997_Schuster and Paliwal_Bidirectional recurrent neural networks.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\7WFMR22T\\1997_Schuster and Paliwal_Bidirectional recurrent neural networks.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\4Q9BQTCM\\abs_all.html:text/html}
}

@article{chorowski_end--end_2014,
	title = {End-to-end {Continuous} {Speech} {Recognition} using {Attention}-based {Recurrent} {NN}: {First} {Results}},
	shorttitle = {End-to-end {Continuous} {Speech} {Recognition} using {Attention}-based {Recurrent} {NN}},
	url = {http://arxiv.org/abs/1412.1602},
	abstract = {We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.},
	urldate = {2014-12-05},
	journal = {arXiv:1412.1602 [cs, stat]},
	author = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {00000 arXiv: 1412.1602},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: As accepted to: Deep Learning and Representation Learning Workshop, NIPS 2014},
	file = {2014_Chorowski et al._End-to-end Continuous Speech Recognition using Attention-based Recurrent NN Fir.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\3B58B3GI\\2014_Chorowski et al._End-to-end Continuous Speech Recognition using Attention-based Recurrent NN Fir.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\X4U6RE6M\\1412.html:text/html}
}

@inproceedings{mikolov_recurrent_2010,
	address = {Makuhari, Chiba, Japan},
	title = {Recurrent neural network based language model},
	url = {http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf},
	urldate = {2014-12-04},
	author = {Mikolov, Tomás and Karafiát, Martin and Burget, Lukás and Cernocky, Jan and Khudanpur, Sanjeev},
	month = sep,
	year = {2010},
	note = {00000},
	file = {2010_Mikolov et al._Recurrent neural network based language model.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\5JUAAFKZ\\2010_Mikolov et al._Recurrent neural network based language model.pdf:application/pdf}
}

@article{cho_properties_2014,
	title = {On the {Properties} of {Neural} {Machine} {Translation}: {Encoder}-{Decoder} {Approaches}},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1409.1259},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	urldate = {2015-05-23},
	journal = {arXiv:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {00012 
arXiv: 1409.1259},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	annote = {Comment: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)},
	file = {2014_Cho et al._On the Properties of Neural Machine Translation Encoder-Decoder Approaches.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\9EMR35BR\\2014_Cho et al._On the Properties of Neural Machine Translation Encoder-Decoder Approaches.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\VFVTWMX7\\1409.html:text/html}
}

@article{vinyals_grammar_2014,
	title = {Grammar as a {Foreign} {Language}},
	url = {http://arxiv.org/abs/1412.7449},
	abstract = {Syntactic parsing is a fundamental problem in computational linguistics and Natural Language Processing. Traditional approaches to parsing are highly complex and problem specific. Recently, Sutskever et al. (2014) presented a domain-independent method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem. In this work, we show that precisely the same sequence-to-sequence method achieves results that are close to state-of-the-art on syntactic constituency parsing, whilst making almost no assumptions about the structure of the problem. To achieve these results we need to mitigate the lack of domain knowledge in the model by providing it with a large amount of automatically parsed data.},
	urldate = {2015-05-23},
	journal = {arXiv:1412.7449 [cs, stat]},
	author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
	month = dec,
	year = {2014},
	note = {00004 
arXiv: 1412.7449},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Statistics - Machine Learning},
	file = {2014_Vinyals et al._Grammar as a Foreign Language.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\NNM5US9V\\2014_Vinyals et al._Grammar as a Foreign Language.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\S257FFFN\\1412.html:text/html}
}

@article{sukhbaatar_end--end_2015,
	title = {End-{To}-{End} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1503.08895},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates slightly better performance than RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	urldate = {2015-07-08},
	journal = {arXiv:1503.08895 [cs]},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	month = mar,
	year = {2015},
	note = {00000 
arXiv: 1503.08895},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {2015_Sukhbaatar et al._End-To-End Memory Networks.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\X9AQZRQD\\2015_Sukhbaatar et al._End-To-End Memory Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\HVVTERVJ\\1503.html:text/html}
}

@article{chorowski_attention-based_2015,
	title = {Attention-{Based} {Models} for {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1506.07503},
	abstract = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7\% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18\% PER in single utterances and 20\% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6\% level.},
	urldate = {2015-07-10},
	journal = {arXiv:1506.07503 [cs, stat]},
	author = {Chorowski, Jan and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
	month = jun,
	year = {2015},
	note = {00000 
arXiv: 1506.07503},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {2015_Chorowski et al._Attention-Based Models for Speech Recognition.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\QZDFSE4B\\2015_Chorowski et al._Attention-Based Models for Speech Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\TV5NXI87\\1506.html:text/html}
}

@article{srivastava_dropout:_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	urldate = {2015-11-09},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	note = {00282},
	pages = {1929--1958},
	file = {2014_Srivastava et al._Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\64AFHT3V\\2014_Srivastava et al._Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:application/pdf;Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\5NMFR35X\\srivastava14a.html:text/html}
}

@inproceedings{vinyals_pointer_2015,
	title = {Pointer networks},
	url = {http://papers.nips.cc/paper/5866-pointer-networks},
	urldate = {2016-02-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	year = {2015},
	note = {00010},
	keywords = {Computer Science - Computational Geometry, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {2674--2682},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\XAMF7DB3\\5866-pointer-networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\SGKQJBR3\\1506.html:text/html}
}

@article{kim_character-aware_2015,
	title = {Character-aware neural language models},
	url = {http://arxiv.org/abs/1508.06615},
	urldate = {2016-02-22},
	journal = {arXiv preprint arXiv:1508.06615},
	author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
	year = {2015},
	note = {00011 
bibtex: kim\_character\_2015a},
	file = {2015_Kim et al._Character-aware neural language models.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\5X6E96HC\\1508.06615v4.pdf:application/pdf}
}

@article{srivastava_highway_2015,
	title = {Highway {Networks}},
	url = {http://arxiv.org/abs/1505.00387},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
	urldate = {2016-02-23},
	journal = {arXiv:1505.00387 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	month = may,
	year = {2015},
	note = {00010 
arXiv: 1505.00387},
	keywords = {68T01, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, G.1.6, I.2.6},
	annote = {Comment: 6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop. Full paper is at arXiv:1507.06228},
	file = {2015_Srivastava et al._Highway Networks.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\ASUBUWMX\\2015_Srivastava et al._Highway Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\VCRIJI6E\\1505.html:text/html}
}

@article{jozefowicz_exploring_2016,
	title = {Exploring the {Limits} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	urldate = {2016-02-24},
	journal = {arXiv:1602.02410 [cs]},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	month = feb,
	year = {2016},
	note = {00000 
arXiv: 1602.02410},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1602.02410 PDF:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\QPAVU8UJ\\2016_Jozefowicz et al._Exploring the Limits of Language Modeling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\SEXGZ93Z\\1602.html:text/html}
}

@inproceedings{wroblewska_preliminary_2011,
	title = {Preliminary {Experiments} in {Polish} {Dependency} {Parsing}.},
	url = {http://link.springer.com/content/pdf/10.1007/978-3-642-25261-7.pdf#page=294},
	urldate = {2016-05-12},
	booktitle = {{SIIS}},
	publisher = {Springer},
	author = {Wróblewska, Alina and Wolinski, Marcin},
	year = {2011},
	note = {00012},
	pages = {279--292},
	file = {2011_Wróblewska and Wolinski_Preliminary Experiments in Polish Dependency Parsing..pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\G3JG8RC4\\2011_Wróblewska and Wolinski_Preliminary Experiments in Polish Dependency Parsing..pdf:application/pdf}
}

@article{dickinson_international_????,
	title = {International {Workshop} on {Treebanks} and {Linguistic} {Theories} ({TLT}14)},
	url = {https://pure.knaw.nl/ws/files/1712331/2015_Wouden_TLT14_proceedings.pdf},
	urldate = {2016-05-12},
	author = {Dickinson, Markus and Hinrichs, Erhard and Patejuk, Agnieszka and Przepiórkowski, Adam},
	note = {00000},
	file = {Dickinson et al._International Workshop on Treebanks and Linguistic Theories (TLT14).pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\CMU5HC7A\\Dickinson et al._International Workshop on Treebanks and Linguistic Theories (TLT14).pdf:application/pdf}
}

@article{tiedemann_cross-lingual_2015,
	title = {Cross-{Lingual} {Dependency} {Parsing} with {Universal} {Dependencies} and {Predicted} {PoS} {Labels}},
	url = {http://www.aclweb.org/anthology/W15-21#page=350},
	urldate = {2016-05-12},
	journal = {Depling 2015},
	author = {Tiedemann, Jörg},
	year = {2015},
	note = {00003},
	pages = {340},
	file = {2015_Tiedemann_Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted PoS L.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\DKI98K7J\\2015_Tiedemann_Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted PoS L.pdf:application/pdf}
}

@article{andor_globally_2016,
	title = {Globally {Normalized} {Transition}-{Based} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1603.06042},
	abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
	urldate = {2016-05-13},
	journal = {arXiv:1603.06042 [cs]},
	author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
	month = mar,
	year = {2016},
	note = {00001 
arXiv: 1603.06042},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {2016_Andor et al._Globally Normalized Transition-Based Neural Networks.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\E23JG4QA\\2016_Andor et al._Globally Normalized Transition-Based Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\VFRJPR2R\\1603.html:text/html}
}

@article{nivre_universal_2015,
	title = {Universal {Dependencies} 1.2},
	copyright = {Licence Universal Dependencies v1.2},
	url = {https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548},
	abstract = {Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008).},
	language = {grc},
	urldate = {2016-05-16},
	journal = {http://universaldependencies.github.io/docs/},
	author = {Nivre, Joakim and Agić, Željko and Aranzabe, Maria Jesus and Asahara, Masayuki and Atutxa, Aitziber and Ballesteros, Miguel and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bosco, Cristina and Bowman, Sam and Celano, Giuseppe G. A. and Connor, Miriam and de Marneffe, Marie-Catherine and Diaz de Ilarraza, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Erjavec, Tomaž and Farkas, Richárd and Foster, Jennifer and Galbraith, Daniel and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Goldberg, Yoav and Gonzales, Berta and Guillaume, Bruno and Hajič, Jan and Haug, Dag and Ion, Radu and Irimia, Elena and Johannsen, Anders and Kanayama, Hiroshi and Kanerva, Jenna and Krek, Simon and Laippala, Veronika and Lenci, Alessandro and Ljubešić, Nikola and Lynn, Teresa and Manning, Christopher and Mărănduc, Cătălina and Mareček, David and Martínez Alonso, Héctor and Mašek, Jan and Matsumoto, Yuji and McDonald, Ryan and Missilä, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and Mori, Shunsuke and Nurmi, Hanna and Osenova, Petya and Øvrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Petrov, Slav and Piitulainen, Jussi and Plank, Barbara and Popel, Martin and Prokopidis, Prokopis and Pyysalo, Sampo and Ramasamy, Loganathan and Rosa, Rudolf and Saleh, Shadi and Schuster, Sebastian and Seeker, Wolfgang and Seraji, Mojgan and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simkó, Katalin and Simov, Kiril and Smith, Aaron and Štěpánek, Jan and Suhr, Alane and Szántó, Zsolt and Tanaka, Takaaki and Tsarfaty, Reut and Uematsu, Sumire and Uria, Larraitz and Varga, Viktor and Vincze, Veronika and Žabokrtský, Zdeněk and Zeman, Daniel and Zhu, Hanzhi},
	month = nov,
	year = {2015},
	note = {00005}
}

@article{ling_finding_2015,
	title = {Finding function in form: {Compositional} character models for open vocabulary word representation},
	shorttitle = {Finding function in form},
	url = {http://arxiv.org/abs/1508.02096},
	urldate = {2016-05-16},
	journal = {arXiv preprint arXiv:1508.02096},
	author = {Ling, Wang and Luís, Tiago and Marujo, Luís and Astudillo, Ramón Fernandez and Amir, Silvio and Dyer, Chris and Black, Alan W. and Trancoso, Isabel},
	year = {2015},
	note = {00025},
	file = {2015_Ling et al._Finding function in form Compositional character models for open vocabulary wor.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\U3GRFIZS\\2015_Ling et al._Finding function in form Compositional character models for open vocabulary wor.pdf:application/pdf}
}

@article{nivre_algorithms_2008,
	title = {Algorithms for {Deterministic} {Incremental} {Dependency} {Parsing}},
	volume = {34},
	issn = {0891-2017},
	url = {http://dx.doi.org/10.1162/coli.07-056-R1-07-027},
	doi = {10.1162/coli.07-056-R1-07-027},
	abstract = {Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.},
	number = {4},
	urldate = {2016-05-16},
	journal = {Comput. Linguist.},
	author = {Nivre, Joakim},
	month = dec,
	year = {2008},
	note = {00220},
	pages = {513--553},
	file = {2008_Nivre_Algorithms for Deterministic Incremental Dependency Parsing.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\487CVSQF\\2008_Nivre_Algorithms for Deterministic Incremental Dependency Parsing.pdf:application/pdf}
}

@article{dyer_transition-based_2015,
	title = {Transition-based dependency parsing with stack long short-term memory},
	url = {http://arxiv.org/abs/1505.08075},
	urldate = {2016-05-17},
	journal = {arXiv preprint arXiv:1505.08075},
	author = {Dyer, Chris and Ballesteros, Miguel and Ling, Wang and Matthews, Austin and Smith, Noah A.},
	year = {2015},
	note = {00049},
	file = {P15-1033.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\W454HDRQ\\P15-1033.pdf:application/pdf}
}

@article{tai_improved_2015,
	title = {Improved semantic representations from tree-structured long short-term memory networks},
	url = {http://arxiv.org/abs/1503.00075},
	urldate = {2016-05-17},
	journal = {arXiv preprint arXiv:1503.00075},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	year = {2015},
	note = {00070},
	file = {P15-1150.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\SQFNT49S\\P15-1150.pdf:application/pdf}
}

@article{kiperwasser_simple_2016,
	title = {Simple and {Accurate} {Dependency} {Parsing} {Using} {Bidirectional} {LSTM} {Feature} {Representations}},
	url = {http://arxiv.org/abs/1603.04351},
	abstract = {We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.},
	urldate = {2016-05-17},
	journal = {arXiv:1603.04351 [cs]},
	author = {Kiperwasser, Eliyahu and Goldberg, Yoav},
	month = mar,
	year = {2016},
	note = {00000 
arXiv: 1603.04351},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\ZR34ZQT5\\1603.html:text/html;Kiperwasser_Goldberg_2016_Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\59A4NPDX\\Kiperwasser_Goldberg_2016_Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature.pdf:application/pdf}
}

@article{bowman_fast_2016,
	title = {A {Fast} {Unified} {Model} for {Parsing} and {Sentence} {Understanding}},
	url = {http://arxiv.org/abs/1603.06021},
	abstract = {Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.},
	urldate = {2016-05-17},
	journal = {arXiv:1603.06021 [cs]},
	author = {Bowman, Samuel R. and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D. and Potts, Christopher},
	month = mar,
	year = {2016},
	note = {00000 
arXiv: 1603.06021},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Manuscript under review},
	file = {arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\CPZSJU2E\\1603.html:text/html;Bowman et al_2016_A Fast Unified Model for Parsing and Sentence Understanding.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\WGVG37CA\\Bowman et al_2016_A Fast Unified Model for Parsing and Sentence Understanding.pdf:application/pdf}
}

@inproceedings{swidzinski_towards_2010,
	title = {Towards a bank of constituent parse trees for {Polish}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-15760-8_26},
	urldate = {2016-05-19},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer},
	author = {Świdziński, Marek and Woliński, Marcin},
	year = {2010},
	note = {00031},
	pages = {197--204},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\54DJ67T7\\swiwol10.pdf:application/pdf}
}

@inproceedings{snoek_practical_2012,
	title = {Practical bayesian optimization of machine learning algorithms},
	url = {http://papers.nips.cc/paper/4522-practical},
	urldate = {2016-05-19},
	booktitle = {Advances in neural information processing systems},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	year = {2012},
	note = {00414},
	pages = {2951--2959},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\NF2E2D3U\\4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf:application/pdf}
}

@article{pham_dropout_2013,
	title = {Dropout improves {Recurrent} {Neural} {Networks} for {Handwriting} {Recognition}},
	url = {http://arxiv.org/abs/1312.4569},
	abstract = {Recurrent neural networks (RNNs) with Long Short-Term memory cells currently hold the best known results in unconstrained handwriting recognition. We show that their performance can be greatly improved using dropout - a recently proposed regularization method for deep architectures. While previous works showed that dropout gave superior performance in the context of convolutional networks, it had never been applied to RNNs. In our approach, dropout is carefully used in the network so that it does not affect the recurrent connections, hence the power of RNNs in modeling sequence is preserved. Extensive experiments on a broad range of handwritten databases confirm the effectiveness of dropout on deep architectures even when the network mainly consists of recurrent and shared connections.},
	urldate = {2016-05-19},
	journal = {arXiv:1312.4569 [cs]},
	author = {Pham, Vu and Bluche, Théodore and Kermorvant, Christopher and Louradour, Jérôme},
	month = nov,
	year = {2013},
	note = {00033 
arXiv: 1312.4569},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1312.4569 PDF:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\84KPZ9FR\\2013_Pham et al._Dropout improves Recurrent Neural Networks for Handwriting Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\K49QKN2F\\1312.html:text/html}
}

@inproceedings{le_inside-outside_2014,
	title = {The {Inside}-{Outside} {Recursive} {Neural} {Network} model for {Dependency} {Parsing}.},
	url = {http://www.emnlp2014.org/papers/pdf/EMNLP2014081.pdf},
	urldate = {2016-05-20},
	booktitle = {{EMNLP}},
	author = {Le, Phong and Zuidema, Willem},
	year = {2014},
	note = {00021},
	pages = {729--739},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\X6ZNM2W8\\EMNLP2014081.pdf:application/pdf}
}

@inproceedings{guo_cross-lingual_2015,
	title = {Cross-lingual dependency parsing based on distributed representations},
	volume = {1},
	url = {http://www.anthology.aclweb.org/P/P15/P15-1119.pdf},
	urldate = {2016-05-20},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing}},
	author = {Guo, Jiang and Che, Wanxiang and Yarowsky, David and Wang, Haifeng and Liu, Ting},
	year = {2015},
	note = {00010},
	pages = {1234--1244},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\S552HP6K\\P15-1119.pdf:application/pdf}
}

@article{nivre_maltparser:_2005,
	title = {{MaltParser}: {A} language-independent system for data-driven dependency parsing},
	issn = {1351-3249, 1469-8110},
	shorttitle = {{MaltParser}},
	url = {http://www.journals.cambridge.org/abstract_S1351324906004505},
	doi = {10.1017/S1351324906004505},
	language = {en},
	urldate = {2016-05-20},
	journal = {Natural Language Engineering},
	author = {Nivre, Joakim and Hall, Johan and Nilsson, Jens and Chanev, Atanas and Eryigit, GüLsen and KüBler, Sandra and Marinov, Svetoslav and Marsi, Erwin},
	month = jan,
	year = {2005},
	note = {00666},
	pages = {1},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\2ND6H263\\nle07.pdf:application/pdf}
}

@inproceedings{ballesteros_maltoptimizer:_2012,
	title = {{MaltOptimizer}: an optimization tool for {MaltParser}},
	shorttitle = {{MaltOptimizer}},
	url = {http://dl.acm.org/citation.cfm?id=2380933},
	urldate = {2016-05-20},
	booktitle = {Proceedings of the {Demonstrations} at the 13th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ballesteros, Miguel and Nivre, Joakim},
	year = {2012},
	note = {00043},
	pages = {58--62},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\33TM6E8H\\eacl2012.pdf:application/pdf}
}

@inproceedings{pei_effective_2015,
	title = {An effective neural network model for graph-based dependency parsing},
	url = {http://getao.github.io/papers/dnnparsing.pdf},
	urldate = {2016-05-20},
	booktitle = {Proc. of {ACL}},
	author = {Pei, Wenzhe and Ge, Tao and Chang, Baobao},
	year = {2015},
	note = {00006},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\IWKRBB4J\\P15-1031.pdf:application/pdf}
}

@article{edmonds_optimim_1966,
	title = {Optimim {Branchings}},
	volume = {71B},
	url = {http://nvlpubs.nist.gov/nistpubs/jres/71b/jresv71bn4p233_a1b.pdf},
	number = {4},
	urldate = {2016-05-20},
	journal = {JOURNAL OF RESEARCH of the National Bureau of Standards - B.},
	author = {Edmonds, Jack},
	year = {1966},
	note = {00000},
	pages = {233--240},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\F5FZJQ5A\\jresv71bn4p233_a1b.pdf:application/pdf}
}

@inproceedings{chen_fast_2014,
	title = {A {Fast} and {Accurate} {Dependency} {Parser} using {Neural} {Networks}.},
	url = {http://www-cs.stanford.edu/~danqi/papers/emnlp2014.pdf},
	urldate = {2016-05-17},
	booktitle = {{EMNLP}},
	author = {Chen, Danqi and Manning, Christopher D.},
	year = {2014},
	note = {00188},
	pages = {740--750},
	file = {emnlp2014.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\G8SZ8I8X\\emnlp2014.pdf:application/pdf}
}

@inproceedings{covington_fundamental_2001,
	title = {A fundamental algorithm for dependency parsing},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.7335&rep=rep1&type=pdf},
	urldate = {2016-05-24},
	booktitle = {Proceedings of the 39th annual {ACM} southeast conference},
	publisher = {Citeseer},
	author = {Covington, Michael A.},
	year = {2001},
	note = {00206},
	pages = {95--102},
	file = {covington.pdf:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\7TE6WSI5\\covington.pdf:application/pdf}
}

@article{ballesteros_improved_2015,
	title = {Improved transition-based parsing by modeling characters instead of words with {LSTMs}},
	url = {http://arxiv.org/abs/1508.00657},
	urldate = {2016-09-29},
	journal = {arXiv preprint arXiv:1508.00657},
	author = {Ballesteros, Miguel and Dyer, Chris and Smith, Noah A.},
	year = {2015},
	note = {00034},
	file = {:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\IZSMCHPM\\D15-1041.pdf:application/pdf}
}

@article{ammar_many_2016,
	title = {Many {Languages}, {One} {Parser}},
	volume = {4},
	copyright = {Copyright (c) 2016 Association for Computational Linguistics},
	issn = {2307-387X},
	url = {https://transacl.org/ojs/index.php/tacl/article/view/892},
	language = {en},
	number = {0},
	urldate = {2016-09-29},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Ammar, Waleed and Mulcaire, George and Ballesteros, Miguel and Dyer, Chris and Smith, Noah A.},
	month = jul,
	year = {2016},
	note = {00006},
	pages = {431--444},
	file = {Full Text PDF:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\BD6G9VQU\\2016_Ammar et al._Many Languages, One Parser.pdf:application/pdf;Snapshot:C\:\\Users\\Jan\\Documents\\Papers\\zotero\\storage\\E9Z9C66U\\207.html:text/html}
}