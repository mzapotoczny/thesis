\chapter{Results}

\section{Single language}
All experiments depicted in this section are based on configuration shown in section
~\ref{sec:basic_params}. In table \ref{tab:universal} we show our baseline results
for subset of the UD languages (due to limited computational resources we were
not able to train models for all of them). The results are compared to
SyntaxNet\cite{andor_globally_2016} and ParseySaurus\cite{alberti_parsey_saurus_2017},
both from Google. The ParseySaurus is based on SyntaxNet, and uses characters
as input, while the original SyntaxNet uses word embeddings. Both are transition-based,
and needs no external POS tagger.

\begin{table}[!htbp]
  \centering
  \begin{tabular}{l l | l l | l l | l l}
    language & \#sentences & \multicolumn{2}{c|}{Ours} & \multicolumn{2}{c|}{SyntaxNet} & \multicolumn{2}{c}{ParseySaurus} \\ \hline
    & & UAS & LAS & UAS & LAS & UAS & LAS\\ \hline
    Czech & 87 913 & \textbf{91.41} & \textbf{88.18} & 89.47 & 85.93 & 89.09 & 84.99 \\
    Polish & 8 227 & 90.26 & 85.32 & 88.30 & 82.71 & \textbf{91.86} & \textbf{87.49}\\
    Russian & 5 030 & 83.29 & 79.22 & 81.75 & 77.71 & \textbf{84.27} & \textbf{80.65} \\
    German & 15 892 & 82.67 & 76.51 & 79.73 & 74.07 & \textbf{84.12} & \textbf{79.05}\\
    English & 16 622 & 87.44 & 83.94 & 84.79 & 80.38 & \textbf{87.86} & \textbf{84.45}\\ 
    French & 16 448 & \textbf{87.25} & \textbf{83.50} & 84.68 & 81.05 & 86.61 & 83.1\\
    Ancient Greek & 25 251 & \textbf{78.96} & \textbf{72.36} & 68.98 & 62.07 & 73.85 & 68.1
  \end{tabular}
  \caption{Baseline results of models trained on single languages from
    UD v1.3. Our models use only the orthographic representation of
    tokenized words during inference and works without a separate POS tagger.}
  \label{tab:universal}
\end{table}

It is worth noting that SyntaxNet has different hyperparameters configuration
for each language, while our parser uses the same configuration across all languages.
In the following sections we will show impact of different network parameters
on the result.

\subsection{Impact of \emph{reader} and \emph{predictor}}
Firstly we will inspect the impact of different \emph{reader} and \emph{predictor}
settings.
\begin{table}[!htbp]
  \centering
  \label{tab:results}
  \begin{tabular}{l|cc|cc|cc|}
    training inputs & \multicolumn{2}{c|}{Czech} & \multicolumn{2}{c|}{English} & \multicolumn{2}{c|}{Polish} \\
    & UAS & LAS & UAS & LAS & UAS & LAS \\ 
    \multicolumn{7}{c}{Gold POS tags} \\  \hline
    base word & 
    \textbf{91.7} & \textbf{88} & 
    \textbf{88.6} & 85.1 & 
    \textbf{93.4} & \textbf{89.3} \\
    \multicolumn{7}{c}{Predicted POS tags or no POS tags} \\ \hline
    words & 
    82.4 & 72.1 &
    81.9 & 74.7 & 
    74.6 & 61.6  \\
    chars, soft att. & 
    \textbf{90.1} & 85.7 & % results/inprogress/lab110-01/dependency_norec_smaller_cz/pretraining_best.zip
    86.5 & 82.1 & % results/inprogress/sonata2/dep_en/pretraining_best.zip
    89.1 & 82.5 \\ %results/inprogress/lab110-11/dep_pl_lessclip/pretraining_best.zip
    chars, tags, soft att. & 
    89.6 & 82.8 & % lab110-02 results/inprogress/lab110-02/dependency_norec_smaller_cz_tags/pretraining_best.zip
    86.2 & 81.3 & % results/inprogress/sonata1/dep_en_tags/pretraining_best.zip
    90.4 & 83.9 \\ % results/inprogress/lab110-03/dep_cpw_opt_no_r_tags/pretraining_best.zip
    chars, tags, hard att. & 
    \textbf{90.1} & \textbf{86.7} & % results/inprogress/lab110-01/dependency_norec_smaller_cz_tags_l0_hard_restart/pretraining_best.zip
    \textbf{87.6} & \textbf{83.6} & % sonata2 local_storage/dependency_norec_spbest_en_tags_l0_hard/pretraining_best.zip
    \textbf{91.3} & \textbf{86} \\ % results/inprogress/lab110-17/dependency_norec_smaller_pl_tags_l0_hard/pretraining_best.zip
  \end{tabular}
  \caption{Model performance on selected languages and different training inputs. Experiments were run on UD v 1.2} 
\end{table}

In the upper part "Gold POS tags" we show a favorable situation when we have access
to gold pos tag (approved by human). The inputs are base words embedded in a vector - 
basically instead of whole \emph{reader} part we have just simple table lookup.
This setting allowed us to obtain best results (especially for Polish) but
gold pos tag are not available in real-word setting so this result is noted just
as a trivia.

The words and chars, soft att. settings don't use \emph{predictor}. Using just words
as inputs (embedded in the same manner as base word) gives words results. This is
easy to predict because most words in the dataset occurs only once and so the
network doesn't have clue about role of most of the words.
Instead using character-level input allows us to obtain much better results, especially
because in morphosyntactically rich languages the word spelling catches many
word features that can be used in deducing the pos tag information.



\subsection{Impact of decoding algorithm}


\subsection{Word pieces}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{c c c c}
        \#pieces & UAS & LAS & LAB \\ \hline
        \multicolumn{4}{c}{Polish}\\
        25 & 90.29 &  84.91 &  90.17 \\
        50 & \textbf{90.40} &  \textbf{85.46} &  \textbf{90.77} \\
        75 &  90.23 & 84.87 &  90.40 \\
        100 & 89.97 & 84.44 & 90.23 \\\hline
        \multicolumn{4}{c}{Czech}\\
        25 & 90.29 & 86.50 & 92.66\\
        50 & 90.03 & 86.08 & 92.40\\
        75 & 90.17 & 86.40 & 92.70\\
        100 &90.84 & 87.31 & 93.20
    \end{tabular}
    \caption{Results on word pieces model. \#pieces denotes how many new
    multi-character tokens were used. To convert word to pieces we
    use a greedy algorithm in which we choose the longest piece that is equal
    to prefix of word.}
    \label{tab:word_pieces}
\end{table}


\subsection{Pointer softening}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{c c c c c}
        Language & Algorithm & UAS & LAS & LAB \\ \hline
        \multirow{2}{*}{Polish}& Greedy & 90.91\% & 86.18\% & 91.16\% \\
               & Edmonds & 90.90\% & 86.15\% & 91.16\% \\ \hline
        \multirow{2}{*}{Czech}& Greedy & 90.38\% & 86.79\% & 92.79\% \\
               & Edmonds & 90.37\% & 86.77\% & 92.79\% \\
    \end{tabular}
    \caption{Results of label softening. Instead of using one-hot vector as
    groundtruth for the \textit{scorer} we set
    0.51 for the correct position and distribute the rest uniformly}
    \label{tab:soften}
\end{table}


\subsection{Recurrent state size}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{l c c c c}
        Language & \% of recurrent state size & UAS & LAS & LAB \\ \hline \hline
        \multirow{3}{*}{Polish}& 100\% & 90.26 & 85.32 & 90.79 \\
        & 50\% & \textbf{90.54} & \textbf{85.64} & \textbf{90.84} \\
        & 25\% & 89.07 & 82.80 & 89.03 \\ \hline
        \multirow{3}{*}{Czech}& 100\% & \textbf{91.41} & \textbf{88.18} & \textbf{90.79} \\
        & 50\% & 89.98 & 85.92 & 92.31\\
        & 25\% & 85.56 & 79.16 & 87.88\\ \hline% ale tutaj to malo epok bylo
    \end{tabular}
    \label{tab:birnn_single_size}
    \caption{Impact of the \emph{tagger} recurrent state size on the performance.
    We report this value as percent of baseline rnn size.}
\end{table}

\section{Multilanguage}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{l c c c c}
        \% of recurrent state size & UAS & LAS & LAB \\ \hline 
        200\% & 91.45 & 86.36& 91.05 \\
        100\% & \textbf{91.65} & \textbf{86.88} & \textbf{91.57}\\
        50\% & 89.53 & 83.66 & 89.55 \\
        25\% & 87.28 & 78.93 & 85.41
    \end{tabular}
    \label{tab:birnn_multi_size}
    \caption{Impact of the \emph{tagger} recurrent state size on the performance
    of multilanguage pl-cs model.
    We report this value as percent of baseline rnn size.}
\end{table}

\section{Error analysis}

\begin{figure}[!htbp]
  \centering
  \resizebox{\textwidth}{!}{
    \import{img/examples/cle/}{cle.pdf_tex}
  }
  \caption{CLE Example} 
  \label{fig:cle}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \resizebox{1\textwidth}{!}{
    \import{img/examples/pos/}{bogumil.pdf_tex}
  }
  \caption{The model architecture.} 
  \label{fig:architecture}
\end{figure}
\begin{figure}[!htbp]
  \centering
  \resizebox{1\textwidth}{!}{
    \import{img/examples/pos/}{michal.pdf_tex}
  }
  \caption{The model architecture.} 
  \label{fig:architecture}
\end{figure}
