\chapter{Results}

\section{Single language}
All experiments depicted in this section are based on configuration shown in section
~\ref{sec:basic_params}. In table \ref{tab:universal} we show our baseline results
compared to state-of-the-art SyntaxNet\cite{andor_globally_2016}
and recently relased ParseySaurus\cite{alberti_parsey_saurus_2017}.

\begin{table}[!htbp]
  \centering
  \begin{tabular}{l l | l l | l l | l l}
    language & \#sentences & \multicolumn{2}{c|}{Ours} & \multicolumn{2}{c|}{SyntaxNet} & \multicolumn{2}{c}{ParseySaurus} \\ \hline
    & & UAS & LAS & UAS & LAS & UAS & LAS\\ \hline
    Czech & 87 913 & \textbf{91.41} & \textbf{88.18} & 89.47 & 85.93 & 89.09 & 84.99 \\
    Polish & 8 227 & 90.26 & 85.32 & 88.30 & 82.71 & \textbf{91.86} & \textbf{87.49}\\
    Russian & 5 030 & 83.29 & 79.22 & 81.75 & 77.71 & \textbf{84.27} & \textbf{80.65} \\
    German & 15 892 & 82.67 & 76.51 & 79.73 & 74.07 & \textbf{84.12} & \textbf{79.05}\\
    English & 16 622 & 87.44 & 83.94 & 84.79 & 80.38 & \textbf{87.86} & \textbf{84.45}\\ % Zblizamy sie do saurusa a dopiero pratraining...
    French & 16 448 & \textbf{87.25} & \textbf{83.50} & 84.68 & 81.05 & 86.61 & 83.1\\
    Ancient Greek & 25 251 & \textbf{78.96} & \textbf{72.36} & 68.98 & 62.07 & 73.85 & 68.1
  \end{tabular}
  \caption{Baseline results of models trained on single languages from
    UD v1.3. Our models use only the orthographic representation of
    tokenized words during inference and works without a separate POS tagger.}
  \label{tab:universal}
\end{table}

predictor, reader impact

\subsection{Impact of \emph{reader} and \emph{predictor}}
words vs chars-per-word + bez predictora

\subsection{Gold POS tags}


\subsection{Soft vs hard attention}


\subsection{Impact of decoding algorithm}


\subsection{Word pieces}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{c c c c}
        \#pieces & UAS & LAS & LAB \\ \hline
        \multicolumn{4}{c}{Polish}\\
        25 & 90.29\%  & 84.91\%  & 90.17\% \\
        50 & 90.40\%  & 85.46\%  & 90.77\% \\
        75 & 90.23\%  & 84.87\%  & 90.40\% \\
        100 & 89.97\%  & 84.44\%  & 90.23\% \\
        \multicolumn{4}{c}{Czech}\\
        25 & \textit{90.29}& \textit{86.50} & \textit{92.66}\\
        50 & \textit{90.03} & \textit{86.08} & \textit{92.40} \\
        75 & \textit{90.06} & \textit{86.32} & \textit{92.71} \\
        100 & \textit{90.10} & \textit{86.17} & \textit{92.41} \\
    \end{tabular}
    \caption{Results on word pieces model. \#pieces denotes how many new
    multi-character tokens were used. To convert word to pieces we
    use a greedy algorithm in which we choose the longest piece that is equal
    to prefix of word.}
    \label{tab:word_pieces}
\end{table}


\subsection{Pointer softening}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{c c c c c}
        Language & Algorithm & UAS & LAS & LAB \\ \hline
        Polish & Greedy & 90.91\% & 86.18\% & 91.16\% \\
               & Edmonds & 90.90\% & 86.15\% & 91.16\% \\
        Czech  & Greedy & 90.38\% & 86.79\% & 92.79\% \\
               & Edmonds & 90.37\% & 86.77\% & 92.79\% \\
    \end{tabular}
    \caption{Results of label softening. Instead of using one-hot vector as
    groundtruth for the \textit{scorer} we set
    0.51 for the correct position and distribute the rest uniformly}
    \label{tab:soften}
\end{table}


\subsection{Recurrent state size}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{l c c c c}
        Language & \% of recurrent state size & UAS & LAS & LAB \\ \hline 
        Polish & 100\% & 90.26 & 85.32 & 90.79 \\
        Polish & 50\% & \textbf{90.54} & \textbf{85.64} & \textbf{90.84} \\
        Polish & 25\% & 89.07 & 82.80 & 89.03 \\ \hline
        Czech & 100\% & 91.41 & 88.18 & 90.79 \\
        Czech & 50\% & 89.98 & 85.92 & 92.31\\
        Czech & 25\% & 85.56 & 79.16 & 87.88\\ \hline% ale tutaj to malo epok bylo
    \end{tabular}
    \label{tab:birnn_single_size}
    \caption{Impact of the \emph{tagger} recurrent state size on the performance.
    We report this value as percent of baseline rnn size.}
\end{table}

\section{Multilanguage}

\begin{table}[!htbp]
    \centering
    \begin{tabular}{l c c c c}
        \% of recurrent state size & UAS & LAS & LAB \\ \hline 
        200\% & 91.45 & 86.36& 91.05 \\
        100\% & 91.65 & 86.88 & 91.57\\
        50\% & 89.53 & 83.66 & 89.55 \\
        25\% & 87.28 & 78.93 & 85.41
    \end{tabular}
    \label{tab:birnn_multi_size}
    \caption{Impact of the \emph{tagger} recurrent state size on the performance
    of multilanguage pl-cs model.
    We report this value as percent of baseline rnn size.}
\end{table}

\section{Error analysis}
